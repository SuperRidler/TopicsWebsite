<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
		<title></title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8;"/>
        <meta name="description" content="Comouting in Surgery, Image Guidance." />
        <link rel="stylesheet" href="css/style_vertical.css" type="text/css" media="screen"/>
		<link rel="stylesheet" href="css/button.css" type="text/css"/>
		<style type ="text/css">
			ts {line-height: 100%; font-size: 22px}
			ul {list-style-type: circle;}
		</style>
    </head>
    <body>
		<div id="top-floating-bar">
            <div id="bar-text"><ul class="nav">
			&nbsp;&nbsp;Computing in Surgery&nbsp;&nbsp;&nbsp;&nbsp;
				<a onClick="goToPage(2);">Introduction</a>&nbsp; / &nbsp; 
                <a onClick="goToPage(3);">Image Registration</a>&nbsp; / &nbsp; 
				<a onClick="goToPage(17);">Point-based Registration</a>&nbsp; / &nbsp; 
                <a onClick="goToPage(25);">Augmented Reality</a>&nbsp; / &nbsp;
				<a onClick="goToPage(33);">Camera Calibration</a>
            </div>
        </div>
		<div id="next-button">
			<a onClick="nextPage();" class="button">Next</a>
		</div>
		<div id="prev-button">
			<a onClick="prevPage();" class="button">Prev</a>
		</div>
        <div class="section white" id="section2">
            <h2>Computing in Surgery - Image Guidance</h2>
            <p>
                Image guidance is key within minimally invasive surgery.<br /><br />
				It provides important information for the surgeon on an as-needs basis, through the use of computer generated image information. <br /><br />
				MIS ensures shorter hospitals stays and smaller risks for the patients.
            </p>
        </div>
		<!-- Image Registration  Starts Page 3 -->
		<div class="section black" id="section3">
            <h2>Image Registration</h2>
			<table width="90%" border="0"
				   style="left:47px">
				<tr>
					<td style="width:50%;">
						<p>Comparing two images that need to be brought into the same spatial coordinates. <br />
						The process of mapping spatial coordinates of one image onto another. <br /></p>
					</td>
					<td style="width:50%;text-align:center;">
						<img src="mimages/ir1.gif" alt="Image Registration Image"/>
					</td>
				</tr>
				<tr>
					<td style="width:50%;text-align:center;">
						<img src="mimages/ir2.jpg" alt="Image Registration Image"/>
					</td>
					<td style="width:50%;vertical-align:top;">
						<p>Two types of registration. <br />
						&nbsp; &nbsp;- Rigid registration               : Image translated, rotated or scaled. <br />
						&nbsp; &nbsp;- Non-rigid registration (Elastic) : Local-level deformation. E.g Regression or growth of disease.</p>
					</td>
				</tr>
			</table>
        </div>
		<div class="section white" id="section4">
            <h2>Image Registration</h2>
            <table width="100%" border="0"
				   style="left:47px">
				<tr>
					<td style="width:50%;font-size:30px;">
						<h3> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Rigid Registration</h3> <br />
						<p>Intensity Based Rigid Registration <br />
						- Iteratively optimises 6 rigid-body parameters describing the orientation of the 3D data-set using a similarity measure. <br />
						Landmark Based Registration <br />
						- Selection of landmarks/feature points and matching them on the images helps to correctly register them. <br />
						Hybrid Registration <br />
						- Tries to incorporate the the benefits of both landmark based registration and intensity based rigid registration. <br /></p>
					</td>
					<td style="width:50%;text-align:center">
						<img src="mimages/ir3.gif" alt="Image Registration Image"/>
					</td>
				</tr>
			</table>
        </div>
		<div class="section black" id="section5">
            <h2>Image Registration</h2>
			<table width="90%" border="0">
				<tr>
					<td style="width:50%;text-align:center">
						<h3>Non-Rigid Registration</h3>
						<img src="mimages/ir3.gif" alt="Image Registration Image"/>
					</td>
					<td style="width:50%;">
						<br />Must use features in the image that are invariant to translation and rotation. 
						These feature points can then register the images using a method called landmark based registration method. <br />
						Average 4-Step Method: <br />
						1. Feature Detection <br />
						2. Feature Matching <br />
						3. Transform Model Estimation <br />
						4. Image Re-Sampling and Transformation <br />
					</td>
				</tr>
			</table>
        </div>
		<div class="section white" id="section6">
			<h2>Image Registration</h2>
			<table width="100%" border="0">
				<tr>
					<td style="width:50%;text-align:center;">
						<img src="mimages/ir4.jpg" alt="Image Registration Image"/>
					</td>
					<td style="width:50%;">
						<h3>Feature Detection</h3> <br />
						Distinctive objects are manually or more preferably automatically detected in the images and can be represented by their point representatives. 
						These points are called control points.<br /> <br />
					</td>
				</tr>
			</table>
        </div>
        <div class="section black" id="section7">
            <h2>Image Registration</h2>
			<table width="100%" border="0"
				   style="left:47px">
				<tr>
					<td style="width:50%;">
						<h3>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Feature Detection</h3> <br />
						<p>In the past objects were manually selected by an expert. Two main approaches to automate this task.<br /> <br />
						Area-Based Methods - More emphasis on the feature matching step than on their detection. First step essentially omitted. <br />
						Feature-Based Methods - Significant regions, lines or points are understood as features here. </p>
					</td>
					<td style="width:50%;">
						<img src="mimages/ir5.jpg" alt="Image Registration Image"/>
					</td>
				</tr>
			</table>
        </div>
		<div class="section white" id="section8">
			 <h2>Image Registration</h2>
			<table width="100%" border="0"
				   style="left:47px">
				<tr>
					<td style="width:50%;font-size:30px;">
						<h3>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Feature Based Methods</h3> <br />
						<p>Features should be spread all over the image and be efficiently detectable in both images. 
						Expected to be stable and stay at fixed positions. 
						Comparability of feature sets is assured by the feature detector. 
						The number of common elements in the detected sets of features should be sufficiently high. 
						Feature-Based Methods do not work directly with image intensity values but represent information on a higher level. 
						This makes the method suitable for different kinds of imaging such as medical imaging.</p>
					</td>
					<td style="width:50%;">
						<img src="mimages/ir6.png" alt="Image Registration Image"/>
					</td>
				</tr>
			</table>
        </div>
		<div class="section black" id="section9">
			<h2>Image Registration</h2>
			<table width="100%" border="0">
				<tr>
					<td style="width:50%;text-align:center;">
						<img src="mimages/ir7.png" alt="Image Registration Image"/>
					</td>
					<td style="width:50%;">
						<h3>Types of Features</h3> <br />
						Region Features : High contrast closed-boundary regions of an appropriate size. <br /> <br />
						Line Features : Representations of general line segments. (Elongated anatomic structures in MI) <br /> <br />
						Point Features : The most distinctive points with respect to a specified measure of similarity. <br /> <br />
					</td>
				</tr>
			</table>
        </div>
		<div class="section white" id="section10">
			<h2>Image Registration</h2>
			<table width="100%" border="0"
				   style="left:47px">
				<tr>
					<td style="width:50%;vertical-align:top;">
						<h3>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Types of Features</h3><br />
						<p>Region Features detected by means of segmentation methods. <br />
						Dependent on the accuracy of the segmentation method. <br />
						Sub-pixel accuracy or registration can be achieved with iterative methods. <br />
						Line Features expressed by pairs of line ends or middle points. <br />
						Line detection can be done by standard edge detection methods like Canny detector or a detector based on the Laplacian of Gaussian.</p>
					</td>
					<td style="width:50%;text-align:center">
						<img src="mimages/ir7.png" alt="Image Registration Image"/>
					</td>
				</tr>
			</table>
        </div>
		<div class="section black" id="section11">
			<h2>Image Registration</h2>
			<table width="90%" border="0">
				<tr>
					<td style="width:50%;text-align:center">
						<img src="mimages/ir8.jpg" alt="Image Registration Image"/>
					</td>
					<td style="width:50%;">
					<h3>Types of Features</h3> <br />
					Point Features detected in most cases by following the definition of a point as a line intersection, centroid of closed-boundary region or local maxima of the wavelet transform. 
					Defining corners is hard. Corner detection methods include SUSAN detector.
					Point Features include corners.  <br />
					Some registration techniques are using both area and feature based approaches.
					</td>
				</tr>
			</table>
        </div>
		<div class="section white" id="section12">
			<h2>Image Registration</h2>
			<table width="100%" border="0">
				<tr>
					<td style="width:50%;">
						<h3>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Feature Matching</h3> <br />
						<p>Matched by means of image intensity values in their close neighbourhoods, the feature spatial distribution, or the feature symbolic description.<br />
						The correspondence between the features in the images is established.</p>
					</td>
					<td style="text-align:center;">
						<br /> <br /> <br />
						<img src="mimages/ir9.jpg" alt="Image Registration Image"/>
					</td>
				</tr>
			</table>
        </div>
		<div class="section black" id="section13">
			<h2>Image Registration</h2>
			<table width="100%" border="0">
				<tr>
					<td style="width:50%;text-align:center;">
						<br />
						<img src="mimages/ir10.jpg" alt="Image Registration Image"/>
					</td>
					<td style="width:50%;">
					<h3>Feature Matching</h3> <br />
					Area-Based Methods - Suits the registration of images which locally differ only by a translation. <br /> <br />
					Any images deformed by more complex transformations will not register properly. <br /><br />
					Another disadvantage is that there is a high chance that any two non-detailed areas will be matched incorrectly. 
					</td>
				</tr>
			</table>
        </div>
		<div class="section white" id="section14">
			<h2>Image Registration</h2>
			<table width="100%" border="0"
				   style="left:47px">
				<tr>
					<td style="width:50%;">
						<h3>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Feature Matching</h3> <br />
						<br />
						<p>Correlation-Like Methods - Classic area-based methods including cross-correlation try to matching image intensities without structural analysis. <br /> <br />
						This makes it very sensitive to intensity changes such as noise.</p>
					</td>
					<td style="width:50%;text-align:center;">
						<br />
						<img src="mimages/ir11.png" alt="Image Registration Image"/>
					</td>
				</tr>
			</table>
        </div>
		<div class="section black" id="section15">
			<h2>Image Registration</h2>
			<table width="90%" border="0">
				<tr>
					<td style="width:50%;text-align:center;">
						<br />
						<img src="mimages/ir12.jpg" alt="Image Registration Image"/>
					</td>
					<td style="width:50%;font-size:29px;">
						<h3>Feature Matching</h3> <br />
						Fourier Methods - Preferred when there is varying conditions (varying light intensity) or the images are corrupted with noise. 
						Phase correlation method is based on the Fourier Shift Theorem. <br />
						This computes the cross-power spectrum of the images and looks for the location of the peak in its inverse. <br />
						This method has robustness against noise and illumination levels. <br />
						Time savings are more significant as the image size increases. 
						In case of varying image scales, combine polar-log mapping and phase correlation.
					</td>
				</tr>
			</table>
        	</div>
		<div class="section white" id="section16">
			<h2>Image Registration</h2>
			<table width="90%" border="0"
				   style="left:47px">
				<tr>
					<td style="width:50%;font-size:22px;">
						<h3>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Feature Matching</h3>
						<p>Mutual Information Methods - Leading technique in multi modal registration. Required for medical imaging. 
						Comparison of anatomical and functional images of the patients body can lead to a a diagnosis that would be impossible to gain otherwise. 
						Remote sensing allows for the exploitation of other sensor types too.</p>
					</td>
					<td style="width:50%;text-align:center;">
						<img src="mimages/ir13.jpg" alt="Image Registration Image"/>
					</td>
				</tr>
				<tr>
					<td style="width:50%;text-align:center;">
						<img src="mimages/ir14.png" alt="Image Registration Image"/>
					</td>
					<td style="width:50%;font-size:22px;">
						Originates in information theory, measures statistical dependency between two data sets and is particularly suitable for different modalities. <br />
						This method is based upon the maximum of MI. Can be sped up by using the coarse-to-fine resolution strategy (the pyramidal approach). <br />
						Can be used to match MRI images and 3D object model matching to the real scene. Gradient Descent Optimisation. 
					</td>
				</tr>
			</table>
		</div>
		<div class="section black" id="section17">
			<h2>Point-based Registration</h2>
			<table width="100%" border="0"
				   style="border:0px solid white;
						   left:250px">
				<tr>
					<td style="width:100%;font-size:26px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Point-based image registration</h2>
						<p style="width:850px;">The Procrustes problem from linear algebra is a matrix approximation problem. <br />
						In a more general sense, it concerns methods that are used to transform one set of data to represent another set of data as closely as possible. <br />
						It was named after the greek robber Procrustes, who made his victims fit into his bed by either stretching them or cutting their legs off. <br />
						This is, very roughly, what we want to do with images as well. Sometimes this process of image registration is also called Spatial Normalization. <br />
					</td>
				</tr>
			</table>
		</div>
		<div class="section white" id="section18">
			<h2>Point-based Registration</h2>
			<table width="90%" border="0"
				   style="border:0px solid white;left:47px">
				<tr style="vertical-align:top;">
					<td style="width:50%;font-size:22px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;How does it apply to image guidance?</h2>
						<p>With a given three-dimensional model of the patient, it is not always easy to project it onto the patient. For example, he might be in a different position from when the scan was taken. Even if the tissue shapes match closely, we still need to find the right translation and rotation parameters to successfully map the scan onto the patient. In more difficult cases, an object of interest such as a tumour might have shrunk or grown since the scan, or there might be new tumours that appeared recently. A good projection system should take into account all such transformations or changes of tissue and still map the scan data in a satisfying way. The problems can be summarized into the following categories: </p>
						<p>- Motion correction <br />- Distortion correction <br /> - Alignment of images obtained at different times or with different imaging parameters</p>					
					</td>
					<td width="5%"/>
					<td style="width:50%;font-size:22px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Methods to solve the problem</h2>
						<p>Given that we have some form of similarity criteria, in this case usually points of interest in the image, we need to find a transformation model that satisfies these criteria well enough and then possibly optimize this model further. A first step is smoothing and re-sampling of the images, and then extracting features. We now have a measure of similarity, and can start computing the actual transformation.</p>
					</td>
				</tr>
			</table>
		</div>
		<div class="section black" id="section19">
			<h2>Point-based Registration</h2>
			<table width="90%" border="0"
				   style="border:0px solid white;left:47px">
				<tr>
					<td style="width:100%;font-size:26px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Transformation Models</h2>
						<p style="width:100%;">Most commonly named are rigid and non-rigid or elastic transformations, although there are semi-elastic or affine transformations that are used to approximate elastic transformations.<br />
						Rigid transformations are linear (i.e. can be described by a matrix). They consist of 3 rotations and 3 translations. This model can be refined by including stretches and shears, and still stays linear. That way, a non-rigid transformation can be crudely approximated. However, this crude approximation is not quite good enough – in practice it’s just not always accurate and thus we have to look for better ways of simulating transformations in the tissue.<br />
						In most applications, where the actual patient anatomy might not match the reference models available, or where there is a lot of distortion in the images, we must use a non-rigid transformation. Non-rigid transformations are generally considered to yield much better results than affine or rigid ones, and are based on spline or similar parameterisations.<br />
						The main goal with these transformation models is to get as much control over the degree of non-rigidity as possible. Otherwise, results can be very unstable and small distortions in the data could possibly create larger unwanted structures. Usually, this can be controlled by for example smoothness constraints such as modelling strain energy, or through a limited number of parameters especially in spline-based approaches.</p>
					</td>
				</tr>
			</table>
		</div>
		<div class="section white" id="section20">
			<h2>Point-based Registration</h2>
			<table width="90%" border="0"
				   style="border:0px solid white;left:47px">
				<tr style="vertical-align: top;">
					<td style="width:50%;font-size:22px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; B-spline based non-rigid registration</h2>
						<p>In this section, we will shortly introduce an algorithm for non-rigid registration developed by Rueckert et al. 1999.
						It uses an affine model to compensate for global motion of the subject (in this case the breast). 
						Local changes are modelled using a free-form deformation model based on B-splines. 
						The basic idea is to manipulate an underlying mesh of control points to produce a smooth transformation. </p>					
					</td>
					<td width="5%"/>
					<td style="width:50%;font-size:22px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; What is a B-spline?</h2>
						<p> B-splines are, very roughly speaking, localized Bezier curves. Only neighbouring locations are considered for every point – thus the computational complexity remains low even for a high sampling rate. Another advantage is that they keep more to the original structure – even if other data points might be significantly distorted these will only affect the transformation in their direct neighbourhood.
						<br/>The described properties make B-splines very suitable to model human tissue. They also give a certain amount of flexibility depending on what kind of tissue is modelled – less control points are best for modelling global non-rigid transformations, while many control points allow for highly local transformations. </p>
					</td>
				</tr>
			</table>
		</div>
		<div class="section black" id="section21">
			<h2>Point-based Registration</h2>
			<table width="90%" border="0" style="border:0px solid white; left:47px">
				<tr style="vertical-align: top;">
					<td style="width:50%;font-size:22px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; How do they work, mathematically?</h2>
						<p>B-splines have so called basis functions. In the algorithm described here, described in Rueckert et al. 1999, the following are used:</p>
					</td>
					<td width="5%"/>
					<td style="width:50%;font-size:22px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; B-spline basis functions</h2>
						<img src="haskell/formula1.png" alt="Formula 1"/>
					</td>
				</tr>
			</table>
		</div>
		<div class="section white" id="section22">
			<h2>Point-based Registration</h2>
			<table width="90%" border="0"
				   style="border:0px solid white; left:47px">
				<tr style="vertical-align: top;">
					<td style="width:50%;font-size:22px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Free-form deformations</h2>
						<p>From these B-splines, we can then define the free-form deformation on the object. Free-form deformations are techniques to manipulate any shape – simply put, maps between vector spaces that define a new position for every point in the given region. 
						We can get coordinates in the transformed vector space using the 3D (since we are in a 3D space) tensor product equation, Where p<sub>(i,j,k)</sub> denotes a control point and <img src="haskell/formula3.png" alt="Formula 3"/> i.e. normalised coordinates. Note that a custom weight can be assigned to each coordinate to gain more local control over the transformation. It is suggested that this weight penalizes non-rigid transformations and doesn’t affect rigid (i.e. rotation and translation) changes.</p>
					</td>
					<td width="5%"/>
					<td style="width:50%;font-size:22px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Formulae</h2>
						<img src="haskell/formula2.png" alt="Formula 2"/>
						<br/><img src="haskell/formula3.png" alt="Formula 3"/>
					</td>
				</tr>
			</table>
		</div>
		<div class="section black" id="section23">
			<h2>Point-based Registration</h2>
			<table width="90%" border="0"
				   style="border:0px solid white; left:47px">
				<tr>
					<td style="width:100%;font-size:22px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Refining the transformation</h2>
						<p>Once the transformed grid coordinates are computed, these can either be refined further or used to create the output. This step consists simply of linear or cubic interpolation to get continuous data from the grid. If further refinement needs to be done, Rueckert et al. suggest that a hierarchy based combination of multiple FFDs at different resolutions and grid sampling rates is used. This will usually lead to even better results. </p>
					</td>
				</tr>
				<tr>
					<td style="width:100%;font-size:22px;text-align:center;">
					<br /> <br />
						<img src="haskell/haskell1-inverted.png" alt="Haskell sucks"/>
					</td>
				</tr>
			</table>
		</div>
		<div class="section white" id="section24">
			<h2>Point-based Registration</h2>
			<table width="90%" border="0" style="border:0px solid white;left:47px">
				<tr>
					<td style="width:100%;font-size:22px;">
						<h2>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Optimal transformations</h2>
						<p>An important part of this algorithm is the aforementioned similarity measure. This is applied to find the optimal affine transformation parameters before iterating through the different resolution levels of the B-spline grid transformation. Then during the iterations to find a non-rigid transformation, it is used as part of a cost function to determine when the transformation is good enough and the algorithm should terminate.</p>
					</td>
				</tr>
				<tr>
					<td style="width:100%;font-size:22px;text-align:center;">
						<br/><img src="haskell/bsplines.gif" alt="B-Spline transformation"/>
					</td>
				</tr>
			</table>
		</div>
		<div class="section black" id="section25">
			<h2>Augmented Reality</h2>
			
			<table width="90%" style="border:0px solid white;left:47px">
			<tr>
				<td>
					<p>With the increased use of robotic assisted, minimally invasive, surgery this has become the ideal environdment for such things as Augmented Reality. <br />
					Traditional AR methods, however, such as overlaid AR, have suffered from a lack of depth perception. <br />
					Even when the objects in question are rendered at the correct depth, the brain perceives the virtual images to be floating, thus hampering the process.</p>
				</td>
				
				<td>
					<img src="augimages/gr4-manualnavigation.jpg" alt="Manual Navigation Image"/>
				</td>
			</tr>
			</table>
        </div>
		<div class="section white" id="section26">
			<h2>Augmented Reality</h2>			
			<table width="100%" style="border:0px solid white; left:47px">
			<tr>
				<td>
					<p style="width:100%;">To perceive the image as embedded the brain expects some form of occlusion/obstruction.<br />
					As such AR allowed the surgeon to view the medical imagery from a natural viewpoint. <br />
					<br />
					&nbsp; &nbsp; &nbsp; &nbsp;<b>Positives</b>;<br />
					 - The effectiveness and clinical benefit of AR has been widely recognised
					</td>
			</tr>
			<tr>
				<td>
				<p style="width:100%;">&nbsp; &nbsp; &nbsp; &nbsp;<b>Negatives</b>;<br />
					 - The application of the technique to cardiothoracic<br /> 
					&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; or gastrointestinal surgery remains limited<br />
					 - Tissue deformation can present significant challenge to AR displays<br /></p>
				</td>
			</tr>
			</table>
        </div>
		<div class="section black" id="section27">
			<h2>Augmented Reality</h2>
			<table width="90%"
				   style="border:0px solid white; left:47px;">
			<tr>
				<td>
				<p style="width:80%;">Techniques such as P-Q Space Rendering provide see-through vision of embedded virtual objects.
				These maintain the projected anatomical detail of the eposed surface whilst aiding the surgery.<br /></p>
				</td>
			</tr>
			<tr>
				<td style="font-size:10px;text-align:center;">
					<br /> <br /> <br />
					<img src="augimages/PQ.jpg" alt="PQ Space Rendering"/><br />
					images curtosy of: http://www.cs.rochester.edu/~bh/igs/
				</td>
			</tr>
			</table>
        </div>
		<div class="section white" id="section28">
			<h2>Augmented Reality</h2>
			<table width="90%"
				   style="border:0px solid white;left:47px">
			<tr>
				<td>
				<p style="width:80%">The idea of P-Q Space Rendering is to render the exposed surface as a translucent layer whilst ensuring that sufficient detail remains to aid both navigation and depth cue-ing. <br />
				The surface geometry is based on p-q space representation. 'p' and 'q' represent the slope of the surface along the x and y axes, respectively.</p>
				</td>
				<td style="text-align:center;">
					<img src="augimages/VR.jpg" alt="Good VR Examples"/>
				</td>
			</tr>
			</table>
        </div>
		<div class="section black" id="section29">
			<h2>Augmented Reality</h2>
			<table width="90%"
				   style="border:0px solid white;left:47px">
			<tr>
				<td style="font-size:28px" colspan="2">
				<p style="width:80%;">During broncoscopic and endoscopic procedures, P-Q Space is used heavily alongside the Shape from Shading Algorithm. 
				This is a classical problem within the field of computer vision. 
				It addresses the issue of extracting both surface and relative depth information from a single image.</p>
				</td>
			</tr>
			<tr>
				<td style="font-size:20px" colspan="2"><br /></td>
			</tr>
			<tr>
				<td style="font-size:20px;text-align:center;" colspan="2">
					Below is an example of shape from shading used to pull the depth information of the left image.<br />
					The results have been mathematically protrayed as a graph.
				</td>
			</tr>
			<tr>
				<td colspan="2" style="text-align:center;">
					<img src="augimages/pradosFaceAndReconstruction.jpg" alt="Shape From Shading"/>
				</td>
			</tr>
			<tr>
			<td style="width:25%"></td>
				<td style = "font-size:10px">
					image curtosy of; http://hal.inria.fr/inria-00394230/en/
				</td>
			</tr>
			</table>
        </div>
		<div class = "section white" id = "section30">
			<h2>Augmented Reality</h2>
			<table width="90%"
				   style = "border:0px solid white;left:47px">
			<tr>
				<td>
					<img src = "augimages/equation.jpg" alt = "equation"/>
				</td>				
			</tr>
			<tr>
				<td style = "font-size:28px">
					<p style="width:100%;"><b>E(x,y)</b> represents the image irradiance, the amount of light falling on a surface<br />
					<b>L</b> is reflected radiance of the light-source, the amount of light radiated from a surface<br />
					<b>d</b> is the lens diametre<br />
					<b>f</b> is the focal length<br />
					<b>α</b> is the angle between the optical axis and the light ray going through the centre of the observed angle</p>
				</td>
			</tr>
			</table>
        </div>
		<div class="section black" id="section31">
			<h2>Augmented Reality</h2>
			<table width="100%"
				   style="border:0px solid white; left:47px">
			<tr>
				<td style="font-size:25px;width:60%;">
					<p style="width:90%;">Given the previous equation, α is assumed negliable. However when using endoscopes this cannot be assumed as the camera and light source are assumed to be near the surface.
					As such, the image irradiance in this situation can be described by a constant of the camera parametres, multipled by the surface albedo, the angle between the incident ray and surface normal and a monotomically decreasing function of the surface point and light source.<br />
					<br />
					When using p-q space rendering the surfaces normals for each pixel are extracted (using linear local shape-from-shading algorithms derived from the necessary camera and lighting constraints). 
					Given a 3D tomographic model, the p-q components are then extracted.</p>
				</td>
				<td style="text-align:center;width:50%">
					<img src = "augimages/more PQ.jpg" alt = "more PQ"/>
				</td>
			</tr>			
			</table>
        </div>
		<div class="section white" id="section32">
			<h2>Augmented Reality</h2>
			<table width="90%"
				   style="border:0px solid white; left:47px">
			<tr>
				<td>
					<p style="width:90%;">In P-Q Space representation, the angle between the normal vectors both before and after rigid body registration will remain the same for every surface point.
					This means that local deformation is identified where the angle diverts from the overall mean of the 3D model. <br />
					<br />
					a) A video image of a deformed airway <br />
					b) The PQ deformation map<br />
					c) and d) The 3D rendered images</p>
				</td>
				<td width="25%">
					<img src="augimages/deformation.jpg" alt=" tissue deformation"/>
					<p style="width:90%;font-size:15px;text-align:center;">image curtosy of:<br />
				Medical Image Computing and Computer-assisted Intervention - MICCAI 2003: 6th International Conference, Montréal, Canada, November 2003 : Proceedings </p>
				</td>
			</tr>
			</table>
        </div>
		

		<div class="section black" id="section33">
			<h2>Camera Calibration</h2>
			<table width="100%" border="0">
				<tr>
					<td style="width:55%;text-align:left;vertical-align:top;">
						<h3 style="font-size:30px;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; What is camera calibration?</h3> 
						<p style="width:90%;font-size:22px;">Basically, the process of determining the external(position,orientation) and internal(focal length, image center, scaling factors, lens distortion etc.) parameters of a camera.<br /> <br />
										This is required for creating a mapping between 3-dimensional coordinates(the real world) and 2-dimensional coordinates (the projective plane).<br /><br /> 
										As it might turn out later, this is potentially <u>VERY</u> useful.
						</p> <br />
						<h3 style="font-size:26px;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Uses:</h3>
						<p style="width:90%;font-size:22px;"> - Surgery : more exactly, <u>surgical tracking</u>; <br />
									    - Computer vision - e.g. efficient algorithms for calculating changes in perspective in a virtual 3D space, or more advanced applications(hand-eye coordination); <br />
									    - 3D reconstruction (that is i.e. finding structural properties of 3D objects, such as outer shape, by using the information got out of their projections on a plane - 2D images, to be precise); <br />
						</p>
					</td>
					<td style="width:45%;font-size:15px;text-align:center;vertical-align:center;">
						<br />
						<img src="ccimages/cc1.jpg" alt="Camera Calibration: failed import;"/> <br />
						Copyright:  <a href=http://www.cse.unr.edu/~aerol/projecthome/> http://www.cse.unr.edu/~aerol/projecthome/</a>  .
					</td>
				</tr>
			</table>
        </div>
		

		<div class="section white" id="section34">
			<h2>Camera Calibration</h2>
			<table width="100%" border="0">
				<tr style="vertical-align:top;">
					<td style="width:40%;font-size:15px;">
						<br /> 
						<img src="ccimages/cc2.png" alt="Camera Calibration: failed import;"/> <br />.
					</td>
					<td style="width:60%;font-size:22px;">
						<h3 style="font-size:30px;text-align:center;">How does it work?</h3> <br />
						<p style="width:90%;text-align:justify;">There are several algorithms to choose from, each with its own benefits and drawbacks. However, most of them fit into one of two categories:<br /><br />
						 - Linear methods : all of the equations used to derive the parameters, or conversely, to obtain the coordinates of the 3D object (the so-called ‘feature’) are linear.It doesn’t take into account errors created by lens distortion or alignment => equations are computationally simpler. Hall and Faugeras-Toscani methods emphasize on this approach; <br /><br />
						 - Non-linear methods : equations involve non-linear terms (required for modelling any sort of distortion/scaling effects), resulting in a more complex computation. Zhang’s and Tsai’s work on this sort of algorithms are “influential”. The other major drawback: need for a good initial "guess" of the parameters;<br /><br />
					</p>
					</td>					
				</tr>	
			</table>
			<p style="font-size:22px;width:80%;text-align:left;">One smart approach was to use a combination of both methods in order to reduce the number of necessary takes on the algorithm (can go as high as 5 times more measurements than unknowns), without significant loss in accuracy.
			</p><br />
        </div>
		

		<div class="section black" id="section35">
						<h2>Camera Calibration</h2>
			<table width="100%" border="0">
				<tr style="vertical-align:top">
					<td style="width:50%;font-size:22px;">
						<h3 style="font-size:30px;text-align:left;">How does it work?</h3> <br /><br />
						<h3 style="font-size:26px;text-align:left;">Some food for thought: </h3>
						<p style="width:90%;text-align:left;"> -How do you construct an accurate, reliable model of the typical situation?<br />
										       -How should the camera be mathematically modelled? <br />
										       -Finally, how do you actually put the algorithms in practice? 
						</p>
					</td> 
					<td style="width:50%;text-align:center;font-size:15px;"><br />
						<img src="ccimages/cc4.png"/><br />
						Copyright: <a href=http://en.wikipedia.org/wiki/Pinhole_camera> http://en.wikipedia.org/wiki/Pinhole_camera </a>
					</td>
				</tr>
				<tr>
					<td style="text-align:center;font-size:15px;" >
						<img src="ccimages/cc42.png" alt="Camera Calibration: failed import;"/>
					</td>
					<td style="text-align:left;font-size:22px;">
					<p style="width:90%;"> We can start off with a very simple camera model- the “ideal pinhole camera”.<br />
							       Mainly, we assume that the camera aperture is infinitely small, so that each point in space has only one projective line going through itself and its image (namely, the line that goes through the point and the aperture).<br />
    							       There is a mapping between each point in space and the projective plane(the image) - assuming, of course, that we refer both points in the same system of coordinates; at its simplest, a feature can be represented by a point in space.
					</p>
					</td>
					
				</tr>
			</table>	
        </div>
		
		
		<div class="section white" id="section36">
			<h2>Camera Calibration</h2>
			<h3 style="text-align:left;font-size:30px;"> New way of seeing things! </h3>
			<p style="width:90%;text-align:left;font-size:22px;">Finally, while the equations for this model could be modeled in an Euclidean geometry, we might  (I insist) use an alternative solution, quite  fit for our purpose : <u>homogeneous coordinates</u> and <u>projective geometry</u>.<br /><br />

    									     Things work somewhat like this:<br /><br />
									     - if you represent a point (x, y) in the plane, its homogeneous coordinates are of the form  (xZ, yZ, Z), where Z is a randomly chosen real number (mainly, the set of all values that preserve this [xZ:yZ:Z] ratio <u>forms an equivalence class</u> in 3D space! ); you recover your original coordinates by setting Z to 1 - quite expected;<br />
    									     - alternatively, a point caractherised by 3 (or ,more generally, n ) dimensions in Euclidean Space is caractherised by 4 (n+1, respectively) homogeneous coordinates, which preserve the initial ratio in the Euclidean Space (e.g. : [x1 :x2 : … : xn : 1] );<br /><br />
									     Why does this simplify things in our problem? several reasons:<br />
        								     - imagine that the new coordinate in the homogeneous system (in the case of the 2D point (x,y)) was actually a coordinate in 3D space, according to the 3rd axis.<br />      What would points in the same equivalence class represent?<br />What set would the set of points with fixed 3rd coordinates represent?<br />
        								     - translation is easier representable as a square matrix ; for example, translation of points (x,y) to points (x+t1,y1+t2), with t1 t2 fixed constants, would be modelled by the equation:
				<div style="text-align:center"><img src="ccimages/translationmat.png" alt="Camera Calibration: failed import;" align="center"/></div>
			</p>
        </div>
		

		<div class="section black" id="section37">
			<h2>Camera Calibration</h2>
			<h3 style="text-align:left;font-size:30px;"> New way of seeing things! </h3>
			<p style ="width:90%;font-size:22px;"> - this also means that we can model changes of coordinate system (mainly, a rotation and a translation) by product of matrices; <br/ >
							       - moreover, it’s possible to map points at infinity; there is one point (actually, an equivalence class) for each particular direction; one of the consequences is that you can solve equations involving parallel lines much more naturally;<br />
			</p><br />
			<div style="text-align:center;">
					<img src="ccimages/thumbsup-inverted.jpeg" alt="Camera Calibration: failed import;"/>
					<p style="width:100%;font-size:30px;">We're set!</p>
			</div>
        </div>
		

		<div class="section white" id="section38">
			<h2>Camera Calibration</h2>
			<table width="100%" border="0">
				<tr style="vertical-align:top;font-size:22px;">
					<td style="width:50%;text-align:left;">
						<h3 style="font-size:30px;">What to do?</h3>
						<p style="width:80%;"> Well, whatever our purpose is, we can’t really do anything without finding that camera (or projection) matrix!<br />
   						    - Acting as a mapping between 3D homogeneous coordinates (4-by-1 matrices) and 2D homogeneous coordinates (3-by-1 matrices), our matrix will probably be 3-by-4:<br /><br />
						    Believe it or not, this is the general equation.
						</p>				
					</td>
					<td style="width:50%;text-align:center;"><br /><br /><br />
						<img src="ccimages/generalmateq.gif" alt="Camera Calibration: failed import;"/>
					</td>
				</tr>
			</table>
			<div style="width:100%;font-size:22px;text-align:left;">
				<p style="width:90%;">-Of course, the parameters depend on all possible factors that can influence the mapping of 3D features to the image (potentially very many);<br />

						      -In our simple pinhole camera model we assume just some of these factors to be part of our matrix, and we’ll set some others later on;<br /><br />

						      -<u>External parameters</u>: <u>position</u> of the camera relative to the original system of the projected object; <u>orientation</u> of the camera relative to ... (it will very rarely be the case that the camera’s coordinate system is aligned with that of the object);<br /><br />

-<u>Internal parameters</u>:

        					      -<u>scaling factor</u> (width-height ratio for pixels); <u>focal length</u> (this one is responsible for any sense of perspective in your image, and would be really difficult to model in its natural form);<br />

						      -we’ll avoid in-depth modelling of focal length or distortion in the linear model; keep in mind though that because of our simplifications, higher errors are prone to occur; 
				</p> 
			</div>
        </div>
		
		<div class="section black" id="section39">
			<h2>Camera calibration</h2>
			<table width="100%" border="0">
				<tr style="vertical-align:top;">
					<td colspan="2"style="width:40%;font-size:15px;text-align:center;"><br /><br />
						<img src="ccimages/eq1a.gif" alt="Camera Calibration: failed import;"/><br /><br />
						<img src="ccimages/eq1b.gif" alt="Camera Calibration: failed import;"/><br />
					</td>
					<td style="width:60%;font-size:22px;">
						<h3 style="font-size:30px;text-align:center;">Determining the camera matrix</h3> <br />
						<p style="width:90%;text-align:left;">
							The mathematics is not too complicated - assuming you know how to work with matrices.<br /><br />

							Assuming you had a feature at [x,y,z,1] in the camera’s system (typically you use a checkerboard-like grid called "Tsai grid" to find out the coordinates easily), and that the image plane was at distance f (focal length) from the camera, on the z-axis, one could work out the coodinates of the image point by triangle similarity (equations on left).
						</p>
					</td>
				</tr>
				<tr>
					<td style="width:20%;font-size:15px;text-align:center;"><br />
						<img src="ccimages/ccenter.png" alt="Camera Calibration: failed import;"/><br />
						Copyright: <a href=http://www.umiacs.umd.edu/~ramani/cmsc828d/lecture9.pdf> http://www.umiacs.umd.edu/~ramani/cmsc828d/lecture9.pdf </a>
					</td>
					<td style ="text-align:center;">
						<img src="ccimages/pixeq.gif" alt="Camera Calibration: failed import;"/><br />						
						<img src="ccimages/Kmateq.gif" alt="Camera Calibration: failed import;"/><br />
						<img src="ccimages/alphaeq.gif" alt="Camera Calibration: failed import;"/><br />
					</td>
					<td style="width:60%;font-size:22px;">
						<p style="width:90%;font-size:22px;text-align:left;">
							This, combined with information regarding the image center and pixel size, yields us a new result.<br /><br />
							(x0,y0) represents the image center, and kx and ky are scaling factors for pixels. <br /><br />
							The 3x4 K matrix (on the left) is your most basic calibration matrix.
						</p>
					
					</td>
				</tr>
			</table>
        </div>
		

		<div class="section white" id="section40">
			<h2>Camera calibration</h2>
			<table width="100%" border ="0">
				<tr style="vertical-align:top;">
					<td style="width:60%;font-size:22px;text-align:left;">
						<h3 style="font-size:30px;text-align:left;">Determining the camera matrix</h3> 
						<p style="width:90%">Unfortunately, it is very rare that the coordinate system of the target is aligned to that of the camera. We must first perform two transformations, mainly a rotation and a translation. <br /><br />
								     It turns out the matrix that describes this transformation looks something like this:<br /><br />
								     Therefore, we can multiply the two matrices to obtain our real camera matrix:<br /></br />
								     This projection matrix P has around 11 variable parameters : 5 from K, 3 from R and 3 from the translation vector. There are quite a lot of unknowns!  <br /> 

								     Cool, this is how it looks in theory, but how do we determine it in practice? <br /><br />
								     We use certain points we know about, we project them through the camera we have, and we use the info on the image points to write these equations:<br /><br 
								     Since each pair of real point-image point give us two equations, it would be expected that 6 points be enough to find our parameters. However, due to the possibility of high errors, it is recommended that more points be used. This is called the Direct Linear Method.
						</p>
					</td>
					<td style="width:40%;font-size:15px;text-align:center;"><br /><br /><br />
						<img src="ccimages/RTmat.gif" alt="Camera Calibration: failed import;"><br /><br /><br />
						<img src="ccimages/finalmat.gif" alt="Camera Calibration: failed import;"/><br /><br /><br /><br />
						<img src="ccimages/detmat1.gif" alt="Camera Calibration: failed import;"/><br />
						Copyright: <a href=http://www.peterhillman.org.uk> http://www.peterhillman.org.uk </a>
					</td>
				</tr>
			</table>
        </div>
		<div class="section black" id="section41">
			<h2>Camera calibration</h2>
			<h3 style="font-size:30px;text-align:center;">Is this it?</h3> <br />			
			<p style="width:90%;font-size:22px;text-align:left;">
				Well, nothing should stop you from implementing all this on your computer and start your career as a video game designer.<br />
			</p><br />
			<h3 style="font-size:26px;text-align:left;">Anything else to it?</h3>
			<p style="width:95%;font-size:22px;text-align:left;float:right;">
				Definitely.<br />
					<img src="ccimages/endoscope.jpg"align="right"/><br />
				Uses for camera calibration range widely, from calibrating your personal camera, to modelling computer vision, to surgery, especially surgical tracking.<br />
				- embedded light cameras for tracking position of different instruments, relative to different markers; <br />
    				- small cameras made for endoscopic interventions (calibration must be much more precise), resulting in an efficient and non-invasive way of doing some types of surgery; <br />
				
				- X-ray cameras (Selby's method);<br />
    				- possibly 3D reconstruction of surfaces; <br /><br />
				Many of these application might need an algorithm with a higher precision than the Direct Linear Method. For more info, Tsai's and Zhang's writings provide some non-linear methods for more specific needs. Also there's a large literature on the subject, even on the Internet. Check it out!
			</p>
        </div>
		<div class="section white" id="section42">
			<div class="section white" id="section26">
			<h2>Camera calibration</h2>
			<h3 style="font-size:30px;text-align:left;">Just for the curious</h3> <br />
			<p style="width:90%;font-size:22px;text-align:left;">
				If the last one got your attention, the main algorithm for 3D recovery is quite easy: once you have two cameras (with known projection matrices P1, P2) which observe the same point, you have to solve the equations: <br />
					<img src="ccimages/rec3d.gif" alt="Camera Calibration: failed import;"/><br />
				In something more visual, this translates to finding the intersection of two projective lines [image - epipolar geometry]:<br /><br />
				<div style="text-align:center;font-size:15px"> 				
					<img src="ccimages/2cam3d.jpg" alt="Camera Calibration: failed import;"/><br />
				</div>
			</p>

        </div>		

        <!-- The JavaScript -->
        <!--<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>	-->
		<script type="text/javascript" src="js/jquery.min.js"></script>		
        <script type="text/javascript" src="js/jquery.easing.1.3.js"></script>
        <script type="text/javascript">
		page=2;
		lastPage=42;
		mydiv = document.getElementById('prev-button');
		mydiv.style.visibility = "hidden";
		browserThing();
		onipad = false;
			function goToPage(toPage){
				page = toPage;
				$('html, body').stop().animate({
						scrollTop: $("#section"+page).offset().top}, 1500,'easeInOutExpo');
					if(page==2){
					mydiv = document.getElementById('prev-button');
					mydiv.style.visibility = "hidden";
					mydiv = document.getElementById('next-button');
					mydiv.style.visibility = "visible";
					}else
					if(page==lastPage){
						mydiv = document.getElementById('prev-button');
						mydiv.style.visibility = "visible";
						mydiv = document.getElementById('next-button');
						mydiv.style.visibility = "hidden";
					}else{
						mydiv = document.getElementById('prev-button');
						mydiv.style.visibility = "visible";
						mydiv = document.getElementById('next-button');
						mydiv.style.visibility = "visible";
					}
			}
			function nextPage(){
				if(page!=lastPage){
					page++;
					$('html, body').stop().animate({
						scrollTop: $("#section"+page).offset().top
                    }, 1500,'easeInOutExpo');
				}
				if(page==3){
					mydiv = document.getElementById('prev-button');
					mydiv.style.visibility = "visible"; 
				}else 
				if(page==lastPage){
					mydiv = document.getElementById('next-button');
					mydiv.style.visibility = "hidden";
				}
			}
			function prevPage(){
				if(page!=2){
					page--;
					$('html, body').stop().animate({
						scrollTop: $("#section"+page).offset().top
                    }, 1500,'easeInOutExpo');
				}
				if(page==1){
					mydiv = document.getElementById('prev-button');
					mydiv.style.visibility = "hidden";
				}
				if(page!=lastPage){
					mydiv = document.getElementById('next-button');
					mydiv.style.visibility = "visible";
				}
			}
			function browserThing(){
				if(navigator.platform=='iPad'){
					alert('Sorry! - Not ready for iPad.');
					/*document.getElementById('next-button').style.display='none';
					document.getElementById('prev-button').style.display='none';
					onipad = true;
					var introNext = document.getElementById('intro-next-button');
					introNext.style.position="relative";
					introNext.style.left="825px";
					introNext.style.top="275px";
					introNext.style.visibility="visible";*/
				}
			}
        </script>
    </body>
</html>
